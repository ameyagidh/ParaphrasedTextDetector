{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe embedding for PAWS and MRCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook we are implmenting glove embedding on PAWS and MRCP dataset, detailed instructions on how to run this notebook is provided either in terms of markdown or comments, all the datasets are present in the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the necessary libraries for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-11T20:14:36.999637Z",
     "iopub.status.busy": "2023-04-11T20:14:36.999139Z",
     "iopub.status.idle": "2023-04-11T20:14:37.006074Z",
     "shell.execute_reply": "2023-04-11T20:14:37.005123Z",
     "shell.execute_reply.started": "2023-04-11T20:14:36.999432Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Processing imports\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Model building imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# SKlearn imports\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:14:45.871346Z",
     "iopub.status.busy": "2023-04-11T20:14:45.871005Z",
     "iopub.status.idle": "2023-04-11T20:14:45.955813Z",
     "shell.execute_reply": "2023-04-11T20:14:45.954874Z",
     "shell.execute_reply.started": "2023-04-11T20:14:45.871290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ameyagidh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ameyagidh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloads for string cleaning\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cleaning function for the strings\n",
    "def clean_string(input_str):\n",
    "    \n",
    "    # Lowercase the input_string\n",
    "    input_str = input_str.lower()\n",
    "    \n",
    "    # Remove URLs, links\n",
    "    input_str = re.sub(r\"http\\S+\", \"\", input_str)\n",
    "    input_str = re.sub(r\"www.\\S+\", \"\", input_str)\n",
    "    input_str = re.sub(r\"\\S+@\\S+\", \"\", input_str)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    input_str_punc = \"\".join(char for char in input_str if char not in string.punctuation)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    input_str_stopwords = \" \".join([word for word in re.split('\\W+', input_str_punc) if word not in stopword])\n",
    "    \n",
    "    # Lemmatization\n",
    "    input_str_cleaned = \" \".join([wn.lemmatize(word,'n') for word in re.split('\\W+', input_str_stopwords)])\n",
    "\n",
    "    return input_str_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:15:54.476276Z",
     "iopub.status.busy": "2023-04-11T20:15:54.475934Z",
     "iopub.status.idle": "2023-04-11T20:15:54.482059Z",
     "shell.execute_reply": "2023-04-11T20:15:54.481162Z",
     "shell.execute_reply.started": "2023-04-11T20:15:54.476221Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function is used to load either paws or mrcp dataset, by passing the argument `paws` or `mrcp` to the funtion\n",
    "def load_dataset_for_glove(data):\n",
    "    if data == \"paws\":\n",
    "        df = pd.read_csv('../data/paws/train.csv')\n",
    "        test_data = pd.read_csv('../data/paws/test.csv')\n",
    "    elif data == 'mrcp':\n",
    "        df = pd.read_csv('../data/mrcp-data/msr_paraphrase_train.csv')\n",
    "        test_data = pd.read_csv('../data/mrcp-data/msr_paraphrase_test.csv')\n",
    "    return df, test_data, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-04-11T19:59:43.146172Z",
     "iopub.status.busy": "2023-04-11T19:59:43.145811Z",
     "iopub.status.idle": "2023-04-11T19:59:43.449894Z",
     "shell.execute_reply": "2023-04-11T19:59:43.448980Z",
     "shell.execute_reply.started": "2023-04-11T19:59:43.146114Z"
    }
   },
   "outputs": [],
   "source": [
    "df, test_data, dataset = load_dataset_for_glove('paws') # or load_dataset_for_glove('mrcp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T19:59:44.121952Z",
     "iopub.status.busy": "2023-04-11T19:59:44.121598Z",
     "iopub.status.idle": "2023-04-11T19:59:44.132816Z",
     "shell.execute_reply": "2023-04-11T19:59:44.131772Z",
     "shell.execute_reply.started": "2023-04-11T19:59:44.121894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>In Paris , in October 1560 , he secretly met t...</td>\n",
       "      <td>In October 1560 , he secretly met with the Eng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The NBA season of 1975 -- 76 was the 30th seas...</td>\n",
       "      <td>The 1975 -- 76 season of the National Basketba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>There are also specific discussions , public p...</td>\n",
       "      <td>There are also public discussions , profile sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>When comparable rates of flow can be maintaine...</td>\n",
       "      <td>The results are high when comparable flow rate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>It is the seat of Zerendi District in Akmola R...</td>\n",
       "      <td>It is the seat of the district of Zerendi in A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          sentence1  \\\n",
       "0           0  In Paris , in October 1560 , he secretly met t...   \n",
       "1           1  The NBA season of 1975 -- 76 was the 30th seas...   \n",
       "2           2  There are also specific discussions , public p...   \n",
       "3           3  When comparable rates of flow can be maintaine...   \n",
       "4           4  It is the seat of Zerendi District in Akmola R...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  In October 1560 , he secretly met with the Eng...      0  \n",
       "1  The 1975 -- 76 season of the National Basketba...      1  \n",
       "2  There are also public discussions , profile sp...      0  \n",
       "3  The results are high when comparable flow rate...      1  \n",
       "4  It is the seat of the district of Zerendi in A...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning the sentences for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T19:59:46.771362Z",
     "iopub.status.busy": "2023-04-11T19:59:46.771026Z",
     "iopub.status.idle": "2023-04-11T20:00:14.384059Z",
     "shell.execute_reply": "2023-04-11T20:00:14.383046Z",
     "shell.execute_reply.started": "2023-04-11T19:59:46.771311Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sentence2 = df['sentence2'].astype('str')\n",
    "df.sentence1 = df.sentence1.apply(lambda x: clean_string(x))\n",
    "df.sentence2 = df.sentence2.apply(lambda x: clean_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:00:25.465151Z",
     "iopub.status.busy": "2023-04-11T20:00:25.464814Z",
     "iopub.status.idle": "2023-04-11T20:00:25.476655Z",
     "shell.execute_reply": "2023-04-11T20:00:25.475764Z",
     "shell.execute_reply.started": "2023-04-11T20:00:25.465098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>paris october 1560 secretly met english ambass...</td>\n",
       "      <td>october 1560 secretly met english ambassador n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nba season 1975 76 30th season national basket...</td>\n",
       "      <td>1975 76 season national basketball association...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>also specific discussion public profile debate...</td>\n",
       "      <td>also public discussion profile specific discus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>comparable rate flow maintained result high</td>\n",
       "      <td>result high comparable flow rate maintained</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>seat zerendi district akmola region</td>\n",
       "      <td>seat district zerendi akmola region</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          sentence1  \\\n",
       "0           0  paris october 1560 secretly met english ambass...   \n",
       "1           1  nba season 1975 76 30th season national basket...   \n",
       "2           2  also specific discussion public profile debate...   \n",
       "3           3       comparable rate flow maintained result high    \n",
       "4           4               seat zerendi district akmola region    \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  october 1560 secretly met english ambassador n...      0  \n",
       "1  1975 76 season national basketball association...      1  \n",
       "2  also public discussion profile specific discus...      0  \n",
       "3       result high comparable flow rate maintained       1  \n",
       "4               seat district zerendi akmola region       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:00:29.805176Z",
     "iopub.status.busy": "2023-04-11T20:00:29.804833Z",
     "iopub.status.idle": "2023-04-11T20:00:29.815864Z",
     "shell.execute_reply": "2023-04-11T20:00:29.814708Z",
     "shell.execute_reply.started": "2023-04-11T20:00:29.805123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This was a series of nested angular standards ...</td>\n",
       "      <td>This was a series of nested polar scales , so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>His father emigrated to Missouri in 1868 but r...</td>\n",
       "      <td>His father emigrated to America in 1868 , but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In January 2011 , the Deputy Secretary General...</td>\n",
       "      <td>In January 2011 , FIBA Asia deputy secretary g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Steiner argued that , in the right circumstanc...</td>\n",
       "      <td>Steiner held that the spiritual world can be r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Luciano Williames Dias ( born July 25 , 1970 )...</td>\n",
       "      <td>Luciano Williames Dias ( born 25 July 1970 ) i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          sentence1  \\\n",
       "0           0  This was a series of nested angular standards ...   \n",
       "1           1  His father emigrated to Missouri in 1868 but r...   \n",
       "2           2  In January 2011 , the Deputy Secretary General...   \n",
       "3           3  Steiner argued that , in the right circumstanc...   \n",
       "4           4  Luciano Williames Dias ( born July 25 , 1970 )...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  This was a series of nested polar scales , so ...      0  \n",
       "1  His father emigrated to America in 1868 , but ...      0  \n",
       "2  In January 2011 , FIBA Asia deputy secretary g...      1  \n",
       "3  Steiner held that the spiritual world can be r...      0  \n",
       "4  Luciano Williames Dias ( born 25 July 1970 ) i...      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning data for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:00:31.889597Z",
     "iopub.status.busy": "2023-04-11T20:00:31.889229Z",
     "iopub.status.idle": "2023-04-11T20:00:36.289441Z",
     "shell.execute_reply": "2023-04-11T20:00:36.288489Z",
     "shell.execute_reply.started": "2023-04-11T20:00:31.889538Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data.sentence2 = test_data['sentence2'].astype('str')\n",
    "test_data.sentence1 = test_data.sentence1.apply(lambda x: clean_string(x))\n",
    "test_data.sentence2 = test_data.sentence2.apply(lambda x: clean_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:00:38.509978Z",
     "iopub.status.busy": "2023-04-11T20:00:38.509640Z",
     "iopub.status.idle": "2023-04-11T20:00:38.520760Z",
     "shell.execute_reply": "2023-04-11T20:00:38.519577Z",
     "shell.execute_reply.started": "2023-04-11T20:00:38.509927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>series nested angular standard measurement azi...</td>\n",
       "      <td>series nested polar scale measurement azimuth ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>father emigrated missouri 1868 returned wife b...</td>\n",
       "      <td>father emigrated america 1868 returned wife be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>january 2011 deputy secretary general fiba asi...</td>\n",
       "      <td>january 2011 fiba asia deputy secretary genera...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>steiner argued right circumstance spiritual wo...</td>\n",
       "      <td>steiner held spiritual world researched right ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>luciano williames dia born july 25 1970 brazil...</td>\n",
       "      <td>luciano williames dia born 25 july 1970 former...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          sentence1  \\\n",
       "0           0  series nested angular standard measurement azi...   \n",
       "1           1  father emigrated missouri 1868 returned wife b...   \n",
       "2           2  january 2011 deputy secretary general fiba asi...   \n",
       "3           3  steiner argued right circumstance spiritual wo...   \n",
       "4           4  luciano williames dia born july 25 1970 brazil...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  series nested polar scale measurement azimuth ...      0  \n",
       "1  father emigrated america 1868 returned wife be...      0  \n",
       "2  january 2011 fiba asia deputy secretary genera...      1  \n",
       "3  steiner held spiritual world researched right ...      0  \n",
       "4  luciano williames dia born 25 july 1970 former...      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the max number of words any sentence has, which will be used while converting text to sequences and then to pad, so that length of all the input sentences are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:00.464262Z",
     "iopub.status.busy": "2023-04-11T20:01:00.463921Z",
     "iopub.status.idle": "2023-04-11T20:01:00.492567Z",
     "shell.execute_reply": "2023-04-11T20:01:00.491669Z",
     "shell.execute_reply.started": "2023-04-11T20:01:00.464209Z"
    }
   },
   "outputs": [],
   "source": [
    "s1 = test_data['sentence1']\n",
    "s2 = test_data['sentence2']\n",
    "max_len = 0\n",
    "for i, j in zip(s1, s2):\n",
    "    max_len = max(max_len, max(len(i.split()), len(j.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:01.648816Z",
     "iopub.status.busy": "2023-04-11T20:01:01.648449Z",
     "iopub.status.idle": "2023-04-11T20:01:01.654909Z",
     "shell.execute_reply": "2023-04-11T20:01:01.653888Z",
     "shell.execute_reply.started": "2023-04-11T20:01:01.648763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:06.801853Z",
     "iopub.status.busy": "2023-04-11T20:01:06.801498Z",
     "iopub.status.idle": "2023-04-11T20:01:08.865987Z",
     "shell.execute_reply": "2023-04-11T20:01:08.865023Z",
     "shell.execute_reply.started": "2023-04-11T20:01:06.801795Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(df['sentence1'].values.astype(str))+list(df['sentence2'].values.astype(str)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:08.868261Z",
     "iopub.status.busy": "2023-04-11T20:01:08.867909Z",
     "iopub.status.idle": "2023-04-11T20:01:10.987857Z",
     "shell.execute_reply": "2023-04-11T20:01:10.986920Z",
     "shell.execute_reply.started": "2023-04-11T20:01:08.868210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the text in the 'sentence1' column of the dataframe\n",
    "X_train_q1 = tokenizer.texts_to_sequences(df['sentence1'].values.astype(str))\n",
    "\n",
    "# Pad the sequences in X_train_q1 with zeros to a maximum length of 25\n",
    "# The padding is done after the sequence\n",
    "X_train_q1 = pad_sequences(X_train_q1, maxlen=25, padding='post')\n",
    "\n",
    "# Tokenize the text in the 'sentence2' column of the dataframe\n",
    "X_train_q2 = tokenizer.texts_to_sequences(df['sentence2'].values.astype(str))\n",
    "\n",
    "# Pad the sequences in X_train_q2 with zeros to a maximum length of 25\n",
    "# The padding is done after the sequence\n",
    "X_train_q2 = pad_sequences(X_train_q2, maxlen=25, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:10.990382Z",
     "iopub.status.busy": "2023-04-11T20:01:10.989880Z",
     "iopub.status.idle": "2023-04-11T20:01:11.311022Z",
     "shell.execute_reply": "2023-04-11T20:01:11.309801Z",
     "shell.execute_reply.started": "2023-04-11T20:01:10.990190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the text data from the 'sentence1' column of the test_data dataframe\n",
    "X_testq1 = test_data['sentence1']\n",
    "\n",
    "# Get the text data from the 'sentence2' column of the test_data dataframe\n",
    "X_testq2 = test_data['sentence2']\n",
    "\n",
    "# Convert the text in X_testq1 to numerical sequences using the tokenizer object\n",
    "X_test_q1 = tokenizer.texts_to_sequences(X_testq1.ravel())\n",
    "\n",
    "# Pad the sequences in X_test_q1 with zeros to a maximum length of 25\n",
    "# The padding is done after the sequence\n",
    "X_test_q1 = pad_sequences(X_test_q1, maxlen=25, padding='post')\n",
    "\n",
    "# Convert the text in X_testq2 to numerical sequences using the tokenizer object\n",
    "X_test_q2 = tokenizer.texts_to_sequences(X_testq2.astype(str).ravel())\n",
    "\n",
    "# Pad the sequences in X_test_q2 with zeros to a maximum length of 25\n",
    "# The padding is done after the sequence\n",
    "X_test_q2 = pad_sequences(X_test_q2, maxlen=25, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:11.313698Z",
     "iopub.status.busy": "2023-04-11T20:01:11.313118Z",
     "iopub.status.idle": "2023-04-11T20:01:11.320312Z",
     "shell.execute_reply": "2023-04-11T20:01:11.319595Z",
     "shell.execute_reply.started": "2023-04-11T20:01:11.313495Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:11.322478Z",
     "iopub.status.busy": "2023-04-11T20:01:11.321936Z",
     "iopub.status.idle": "2023-04-11T20:01:37.870339Z",
     "shell.execute_reply": "2023-04-11T20:01:37.869424Z",
     "shell.execute_reply.started": "2023-04-11T20:01:11.322277Z"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "embedding_index = {}\n",
    "with codecs.open('../data/glove.6B.200d.txt',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], 'float32')\n",
    "        embedding_index[word] = vectors\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:01:37.872570Z",
     "iopub.status.busy": "2023-04-11T20:01:37.872188Z",
     "iopub.status.idle": "2023-04-11T20:01:38.014787Z",
     "shell.execute_reply": "2023-04-11T20:01:38.013922Z",
     "shell.execute_reply.started": "2023-04-11T20:01:37.872513Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index)+1, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:02:10.075630Z",
     "iopub.status.busy": "2023-04-11T20:02:10.075251Z",
     "iopub.status.idle": "2023-04-11T20:02:10.081812Z",
     "shell.execute_reply": "2023-04-11T20:02:10.080620Z",
     "shell.execute_reply.started": "2023-04-11T20:02:10.075570Z"
    }
   },
   "outputs": [],
   "source": [
    "y = to_categorical(df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 for sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:04:28.636794Z",
     "iopub.status.busy": "2023-04-11T20:04:28.636374Z",
     "iopub.status.idle": "2023-04-11T20:04:28.913613Z",
     "shell.execute_reply": "2023-04-11T20:04:28.912718Z",
     "shell.execute_reply.started": "2023-04-11T20:04:28.636736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a sequential model for Q1\n",
    "model_q1 = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer to the model with the specified input dimension, output dimension, \n",
    "# weights, and input length\n",
    "model_q1.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                       output_dim=200,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=25))\n",
    "\n",
    "# Add an LSTM layer with 128 units, 'relu' activation function, and return sequences flag set to True\n",
    "model_q1.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "\n",
    "# Add a dropout layer with a rate of 0.2\n",
    "model_q1.add(Dropout(0.2))\n",
    "\n",
    "# Add another LSTM layer with 128 units, 'relu' activation function, and return sequences flag set to True\n",
    "model_q1.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "\n",
    "# Add another dropout layer with a rate of 0.2\n",
    "model_q1.add(Dropout(0.2))\n",
    "\n",
    "# Add a dense layer with 64 units and 'relu' activation function\n",
    "model_q1.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add another dropout layer with a rate of 0.2\n",
    "model_q1.add(Dropout(0.2))\n",
    "\n",
    "# Add a dense layer with 2 units and 'sigmoid' activation function\n",
    "model_q1.add(Dense(2, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 for sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:04:28.915696Z",
     "iopub.status.busy": "2023-04-11T20:04:28.915328Z",
     "iopub.status.idle": "2023-04-11T20:04:29.190574Z",
     "shell.execute_reply": "2023-04-11T20:04:29.189534Z",
     "shell.execute_reply.started": "2023-04-11T20:04:28.915645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a sequential model for Q2\n",
    "model_q2 = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer to the model with the specified input dimension, output dimension, \n",
    "# weights, and input length\n",
    "model_q2.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                       output_dim=200,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=25))\n",
    "\n",
    "# Add an LSTM layer with 128 units, 'relu' activation function, and return sequences flag set to True\n",
    "model_q2.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "\n",
    "# Add a dropout layer with a rate of 0.2\n",
    "model_q2.add(Dropout(0.2))\n",
    "\n",
    "# Add another LSTM layer with 128 units, 'relu' activation function, and return sequences flag set to True\n",
    "model_q2.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "\n",
    "# Add another dropout layer with a rate of 0.2\n",
    "model_q2.add(Dropout(0.2))\n",
    "\n",
    "# Add a dense layer with 64 units and 'relu' activation function\n",
    "model_q2.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add another dropout layer with a rate of 0.2\n",
    "model_q2.add(Dropout(0.2))\n",
    "\n",
    "# Add a dense layer with 2 units and 'sigmoid' activation function\n",
    "model_q2.add(Dense(2, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging both the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:04:29.478110Z",
     "iopub.status.busy": "2023-04-11T20:04:29.477607Z",
     "iopub.status.idle": "2023-04-11T20:04:29.586510Z",
     "shell.execute_reply": "2023-04-11T20:04:29.585602Z",
     "shell.execute_reply.started": "2023-04-11T20:04:29.478040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging the output of the two models, i.e., model_q1 and model_q2\n",
    "# Multiply the output tensors element-wise\n",
    "mergedOut = Multiply()([model_q1.output, model_q2.output])\n",
    "\n",
    "# Flatten the output tensor\n",
    "mergedOut = Flatten()(mergedOut)\n",
    "\n",
    "# Add a fully connected layer with 128 units and ReLU activation\n",
    "mergedOut = Dense(128, activation='relu')(mergedOut)\n",
    "\n",
    "# Apply a dropout of 20% to the previous layer\n",
    "mergedOut = Dropout(0.2)(mergedOut)\n",
    "\n",
    "# Add another fully connected layer with 32 units and ReLU activation\n",
    "mergedOut = Dense(32, activation='relu')(mergedOut)\n",
    "\n",
    "# Apply another dropout of 20% to the previous layer\n",
    "mergedOut = Dropout(0.2)(mergedOut)\n",
    "\n",
    "# Add the final fully connected layer with 2 units and sigmoid activation\n",
    "mergedOut = Dense(2, activation='sigmoid')(mergedOut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:04:30.188550Z",
     "iopub.status.busy": "2023-04-11T20:04:30.185932Z",
     "iopub.status.idle": "2023-04-11T20:04:30.201052Z",
     "shell.execute_reply": "2023-04-11T20:04:30.200088Z",
     "shell.execute_reply.started": "2023-04-11T20:04:30.188480Z"
    }
   },
   "outputs": [],
   "source": [
    "new_model = Model([model_q1.input, model_q2.input], mergedOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Kfold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:04:31.563129Z",
     "iopub.status.busy": "2023-04-11T20:04:31.562789Z",
     "iopub.status.idle": "2023-04-11T20:04:31.568874Z",
     "shell.execute_reply": "2023-04-11T20:04:31.567534Z",
     "shell.execute_reply.started": "2023-04-11T20:04:31.563077Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define number of folds\n",
    "num_folds = 5\n",
    "\n",
    "# Initialize k-fold cross validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "validation_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:04:33.359563Z",
     "iopub.status.busy": "2023-04-11T20:04:33.359157Z",
     "iopub.status.idle": "2023-04-11T20:07:14.535191Z",
     "shell.execute_reply": "2023-04-11T20:07:14.534197Z",
     "shell.execute_reply.started": "2023-04-11T20:04:33.359471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 06:31:00.974644: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.5509WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6884 - accuracy: 0.5509\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.5583WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.6863 - accuracy: 0.5583\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.5583WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.6854 - accuracy: 0.5583\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.5583WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.6845 - accuracy: 0.5583\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.5590WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 25s 1s/step - loss: 0.6822 - accuracy: 0.5590\n",
      "309/309 [==============================] - 11s 33ms/step - loss: 0.6868 - accuracy: 0.5635\n",
      "Validation accuracy: 0.5635057091712952\n",
      "Fold 2:\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.5734WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 25s 1s/step - loss: 0.6809 - accuracy: 0.5734\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 0.5855WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 22s 1s/step - loss: 0.6777 - accuracy: 0.5855\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6729 - accuracy: 0.5971WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.6729 - accuracy: 0.5971\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.6110WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 22s 1s/step - loss: 0.6661 - accuracy: 0.6110\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6552 - accuracy: 0.6220WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 25s 1s/step - loss: 0.6552 - accuracy: 0.6220\n",
      "309/309 [==============================] - 11s 33ms/step - loss: 0.6670 - accuracy: 0.6041\n",
      "Validation accuracy: 0.6041498184204102\n",
      "Fold 3:\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.6175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 30s 1s/step - loss: 0.6551 - accuracy: 0.6175\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6438 - accuracy: 0.6283WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.6438 - accuracy: 0.6283\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.6323WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.6354 - accuracy: 0.6323\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.6364WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.6274 - accuracy: 0.6364\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.6407WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6222 - accuracy: 0.6407\n",
      "309/309 [==============================] - 12s 36ms/step - loss: 0.6476 - accuracy: 0.6163\n",
      "Validation accuracy: 0.616295576095581\n",
      "Fold 4:\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6347 - accuracy: 0.6322WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.6347 - accuracy: 0.6322\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6239 - accuracy: 0.6392WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 30s 1s/step - loss: 0.6239 - accuracy: 0.6392\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6190 - accuracy: 0.6434WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 38s 2s/step - loss: 0.6190 - accuracy: 0.6434\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6147 - accuracy: 0.6460WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.6147 - accuracy: 0.6460\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.6517WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.6098 - accuracy: 0.6517\n",
      "309/309 [==============================] - 13s 41ms/step - loss: 0.6517 - accuracy: 0.5946\n",
      "Validation accuracy: 0.5946356058120728\n",
      "Fold 5:\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.6404WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.6218 - accuracy: 0.6404\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.6456WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.6138 - accuracy: 0.6456\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.6500WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.6078 - accuracy: 0.6500\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6031 - accuracy: 0.6556WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.6031 - accuracy: 0.6556\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.5978 - accuracy: 0.6594WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.5978 - accuracy: 0.6594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309 [==============================] - 11s 34ms/step - loss: 0.6389 - accuracy: 0.6235\n",
      "Validation accuracy: 0.623481810092926\n",
      "\n",
      "Mean validation accuracy: 0.600413703918457\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(X_train_q1, y)):\n",
    "    \n",
    "    print(f\"Fold {fold+1}:\")\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X1_train, X1_val = X_train_q1[train_indices], X_train_q1[val_indices]\n",
    "    X2_train, X2_val = X_train_q2[train_indices], X_train_q2[val_indices]\n",
    "    Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "    \n",
    "    new_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    new_model.fit([X1_train,X2_train],Y_train, batch_size = 64 if dataset == \"mrcp\" else 2000, epochs = 5, verbose=1,\n",
    "                  callbacks=[EarlyStopping(patience=2)])\n",
    "    \n",
    "    # Evaluate model on validation set and store accuracy\n",
    "    scores = new_model.evaluate([X1_val, X2_val], Y_val, verbose=1)\n",
    "    validation_scores.append(scores[1])\n",
    "    \n",
    "    print(f\"Validation accuracy: {scores[1]}\")\n",
    "    \n",
    "mean_accuracy = np.mean(validation_scores)\n",
    "print(f\"\\nMean validation accuracy: {mean_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:07:15.947211Z",
     "iopub.status.busy": "2023-04-11T20:07:15.946867Z",
     "iopub.status.idle": "2023-04-11T20:07:20.479949Z",
     "shell.execute_reply": "2023-04-11T20:07:20.478965Z",
     "shell.execute_reply.started": "2023-04-11T20:07:15.947158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 9s 34ms/step - loss: 0.7255 - accuracy: 0.5579\n",
      "Test accuracy: 0.5578749775886536\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = new_model.evaluate([X_test_q1, X_test_q2], to_categorical(test_data['label'].values))\n",
    "\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T20:07:22.810357Z",
     "iopub.status.busy": "2023-04-11T20:07:22.810016Z",
     "iopub.status.idle": "2023-04-11T20:07:26.930768Z",
     "shell.execute_reply": "2023-04-11T20:07:26.929073Z",
     "shell.execute_reply.started": "2023-04-11T20:07:22.810302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 12s 40ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.84      0.67      4464\n",
      "           1       0.47      0.18      0.26      3536\n",
      "\n",
      "    accuracy                           0.55      8000\n",
      "   macro avg       0.52      0.51      0.47      8000\n",
      "weighted avg       0.52      0.55      0.49      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = new_model.predict([X_test_q1,X_test_q1])\n",
    "\n",
    "y_pred_final = []\n",
    "for x in y_pred:\n",
    "    if x[0] > x[1]:\n",
    "        y_pred_final.append(0)\n",
    "    else:\n",
    "        y_pred_final.append(1)\n",
    "\n",
    "print(classification_report(test_data['label'].values, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This reslts are for paws, but if you just load the data for mrcp as mentioned in the beggining this will give us 63% as testing and 99% as training which again tells us that our data is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the evaluation results i.e. 56% for paws and 63% for mrcp, it appears that the model is overfitting the training data, as indicated by the significantly higher training accuracy compared to the test accuracy. This means that the model is performing well on the training data, but is not generalizing well to new, unseen data.\n",
    "\n",
    "Overfitting occurs when the model is too complex relative to the amount of training data available, or when the model is trained for too many epochs. In this case, it may be necessary to simplify the model architecture, reduce the number of training epochs, or increase the amount of training data.\n",
    "\n",
    "Overall, it is important to take measures to prevent overfitting in order to ensure that the model generalizes well to new data and performs well in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
